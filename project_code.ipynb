{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1d47db",
   "metadata": {},
   "source": [
    "### Topic Modeling using LDA and NMF in Python\n",
    "\n",
    "\n",
    "**Dataset description**: 404,289 Quora questions without any labelled category.\n",
    "    \n",
    "**Objective**: Find 20 categories to assign to the unlabelled questions using unsupervised learning techniques such as Latent   Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f0f8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "##basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc9d0595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##load dataset\n",
    "df = pd.read_csv('quora_questions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7bb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "##null value check\n",
    "total_null = df['Question'].isnull().sum()\n",
    "print(total_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccab3077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "##empty string check\n",
    "total_blank = df['Question'].apply(lambda string: string.isspace()).sum()\n",
    "print(total_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7308d077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404289, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74336fa",
   "metadata": {},
   "source": [
    "### Part 1: Latent Dirichlet Allocation (LDA) based topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf918314",
   "metadata": {},
   "outputs": [],
   "source": [
    "##vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.7, min_df=2, stop_words='english')\n",
    "\n",
    "cv_term_matrix = cv.fit_transform(df['Question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a916f8",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "\n",
    "1) max_df=0.7 => only consider words that appear in less than 70% of the questions\n",
    "2) min_df=2 => only consider words that appear in atleast 2 questions\n",
    "3) stop_words='english' => don't consider way too common english words for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844462e",
   "metadata": {},
   "source": [
    "**Important**: Count vectorization is preferred over TF-IDF here because LDA is based on the Dirichlet Probability Distribution that puts importance on the occurence count of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f34e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=20, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=27, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Latent Dirichlet Allocation\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=20, random_state=27) ##n_components => no. of topics to identify\n",
    "\n",
    "LDA.fit(cv_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58099c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now, the interesting part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "443da698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38669"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##check length of count vectorizer vocabulary\n",
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "036d34f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 38669)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7baab",
   "metadata": {},
   "source": [
    "**Important**: LDA.components_ essentially outputs the normalized distribution for the 20 desired topics over the 38669 words in the vocabulary of the count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac8ad43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: service, police, india, bad, place, safe, laptop, good, buy, best.\n",
      "\n",
      "Topic 1: air, house, beautiful, body, cause, hair, hard, water, long, does.\n",
      "\n",
      "Topic 2: way, earn, free, lose, ways, weight, online, best, money, make.\n",
      "\n",
      "Topic 3: stock, english, make, market, friends, college, stop, girl, best, way.\n",
      "\n",
      "Topic 4: grow, mean, guy, sleep, car, look, feel, like, work, does.\n",
      "\n",
      "Topic 5: visit, friend, places, making, process, age, tell, years, year, old.\n",
      "\n",
      "Topic 6: battle, small, usa, sex, did, history, social, think, start, business.\n",
      "\n",
      "Topic 7: pakistan, black, did, war, india, difference, love, mean, world, does.\n",
      "\n",
      "Topic 8: relationship, important, school, new, going, don, day, things, know, like.\n",
      "\n",
      "Topic 9: current, sentence, machine, human, india, word, real, read, used, life.\n",
      "\n",
      "Topic 10: child, pregnant, asked, sex, period, makes, average, men, women, good.\n",
      "\n",
      "Topic 11: website, movies, increase, hillary, clinton, study, donald, improve, english, trump.\n",
      "\n",
      "Topic 12: india, book, programming, language, way, 1000, notes, 500, learn, best.\n",
      "\n",
      "Topic 13: india, fat, join, working, differences, rid, travel, company, possible, time.\n",
      "\n",
      "Topic 14: difference, data, science, engineering, app, computer, iphone, android, examples, use.\n",
      "\n",
      "Topic 15: meaning, light, countries, united, states, india, experience, bank, exam, prepare.\n",
      "\n",
      "Topic 16: game, favorite, account, com, password, card, phone, india, number, movie.\n",
      "\n",
      "Topic 17: write, universities, new, looking, good, effects, does, university, math, job.\n",
      "\n",
      "Topic 18: good, 2016, election, trump, president, did, energy, happen, win, thing.\n",
      "\n",
      "Topic 19: believe, answer, ask, instagram, question, facebook, questions, google, quora, people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##finding the 10 highest probabilty words for each identified topic\n",
    "\n",
    "lda_dct = dict()\n",
    "\n",
    "for index, topic in enumerate(LDA.components_):\n",
    "    indices = topic.argsort()[-10:] ##grab index positions for 10 highest normalized probability values\n",
    "    words = [cv.get_feature_names()[i] for i in indices] ##use indices to get words from count vectorizer vocabulary\n",
    "    lda_dct[index] = words\n",
    "    \n",
    "##print topics against keywords\n",
    "for topic, keywords in lda_dct.items():\n",
    "    print(f\"Topic {topic}: {', '.join(keywords)}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e91605be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>lda_topic</th>\n",
       "      <th>lda_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>3</td>\n",
       "      <td>stock, english, make, market, friends, college...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>14</td>\n",
       "      <td>difference, data, science, engineering, app, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>14</td>\n",
       "      <td>difference, data, science, engineering, app, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>7</td>\n",
       "      <td>pakistan, black, did, war, india, difference, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>1</td>\n",
       "      <td>air, house, beautiful, body, cause, hair, hard...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  lda_topic  \\\n",
       "0  What is the step by step guide to invest in sh...          3   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...         14   \n",
       "2  How can I increase the speed of my internet co...         14   \n",
       "3  Why am I mentally very lonely? How can I solve...          7   \n",
       "4  Which one dissolve in water quikly sugar, salt...          1   \n",
       "\n",
       "                                        lda_keywords  \n",
       "0  stock, english, make, market, friends, college...  \n",
       "1  difference, data, science, engineering, app, c...  \n",
       "2  difference, data, science, engineering, app, c...  \n",
       "3  pakistan, black, did, war, india, difference, ...  \n",
       "4  air, house, beautiful, body, cause, hair, hard...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##assigning topics to each question alongside representative keywords\n",
    "\n",
    "lda_topic_distribution = LDA.transform(cv_term_matrix)\n",
    "\n",
    "df['lda_topic'] = lda_topic_distribution.argmax(axis=1) ##grab the topic with highest normalized probability\n",
    "df['lda_keywords'] = df['lda_topic'].apply(lambda topic: ', '.join(lda_dct[topic])) ##grab keywords from the lda_dct\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97e890",
   "metadata": {},
   "source": [
    "**Important**: LDA.transform(cv_term_matrix) outputs normalized Dirichlet Probability Distribution for all documents in the corpus gainst the 20 desired topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68ccbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##random lda check function\n",
    "def lda_check(index=np.random.randint(0,df.shape[0])):\n",
    "    print(f\"Topic {df['lda_topic'][index]}: {df['lda_keywords'][index]}.\")\n",
    "    print(\"\\n\")\n",
    "    print(df['Question'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37f653f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 14: difference, data, science, engineering, app, computer, iphone, android, examples, use.\n",
      "\n",
      "\n",
      "Why do some old computer games run very fast on new, powerful computers?\n"
     ]
    }
   ],
   "source": [
    "lda_check(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4e7f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 3: stock, english, make, market, friends, college, stop, girl, best, way.\n",
      "\n",
      "\n",
      "How can one impress girls on Quora?\n"
     ]
    }
   ],
   "source": [
    "lda_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad65af",
   "metadata": {},
   "source": [
    "### Part 2: Non-negative Matrix Factorization (NMF) based topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c909922",
   "metadata": {},
   "outputs": [],
   "source": [
    "##vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.7, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_term_matrix = tfidf.fit_transform(df['Question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582da06",
   "metadata": {},
   "source": [
    "**Important**: TF-IDF vectorization can be used with NMF because it works with coefficients for comparison, unlike LDA that uses normalized Dirichlet probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6651501f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=20, random_state=27, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "NMF = NMF(n_components = 20, random_state=27)\n",
    "\n",
    "NMF.fit(tfidf_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25d14515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: phone, buy, laptop, movie, ways, 2016, books, book, movies, best.\n",
      "\n",
      "Topic 1: use, exist, really, compare, cost, long, feel, work, mean, does.\n",
      "\n",
      "Topic 2: improvement, delete, asked, google, answers, answer, ask, question, questions, quora.\n",
      "\n",
      "Topic 3: internet, free, home, easy, youtube, ways, earn, online, make, money.\n",
      "\n",
      "Topic 4: live, want, change, moment, real, important, thing, meaning, purpose, life.\n",
      "\n",
      "Topic 5: china, business, country, olympics, available, job, spotify, war, pakistan, india.\n",
      "\n",
      "Topic 6: hacking, want, python, languages, java, learning, start, language, programming, learn.\n",
      "\n",
      "Topic 7: vote, better, election, did, win, hillary, president, clinton, donald, trump.\n",
      "\n",
      "Topic 8: place, pakistan, happen, end, country, iii, start, did, war, world.\n",
      "\n",
      "Topic 9: culture, women, work, girls, live, girl, look, sex, feel, like.\n",
      "\n",
      "Topic 10: business, read, start, job, work, engineering, ways, bad, books, good.\n",
      "\n",
      "Topic 11: government, ban, banning, black, indian, rupee, rs, 1000, notes, 500.\n",
      "\n",
      "Topic 12: girl, 2017, year, don, employees, going, day, things, new, know.\n",
      "\n",
      "Topic 13: language, fluently, speak, communication, pronunciation, speaking, writing, skills, improve, english.\n",
      "\n",
      "Topic 14: pounds, reduce, quickly, loss, fast, fat, ways, gain, lose, weight.\n",
      "\n",
      "Topic 15: person, machine, movies, favorite, job, home, sex, possible, travel, time.\n",
      "\n",
      "Topic 16: tell, forget, really, friend, true, know, person, girl, fall, love.\n",
      "\n",
      "Topic 17: increase, painless, instagram, account, best, commit, fastest, suicide, easiest, way.\n",
      "\n",
      "Topic 18: mind, google, flat, questions, hate, believe, ask, don, think, people.\n",
      "\n",
      "Topic 19: better, job, use, account, data, software, science, computer, engineering, difference.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##finding the 10 highest coefficient words for each identified topic\n",
    "\n",
    "nmf_dct = dict()\n",
    "\n",
    "for index, topic in enumerate(NMF.components_):\n",
    "    indices = topic.argsort()[-10:] ##grab indices for the 10 highest coefficients\n",
    "    words = [tfidf.get_feature_names()[i] for i in indices]\n",
    "    nmf_dct[index] = words\n",
    "    \n",
    "##printing topics alongside keywords\n",
    "for topic, keywords in nmf_dct.items():\n",
    "    print(f\"Topic {topic}: {', '.join(keywords)}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ea27195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>lda_topic</th>\n",
       "      <th>lda_keywords</th>\n",
       "      <th>nmf_topic</th>\n",
       "      <th>nmf_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>3</td>\n",
       "      <td>stock, english, make, market, friends, college...</td>\n",
       "      <td>5</td>\n",
       "      <td>china, business, country, olympics, available,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>14</td>\n",
       "      <td>difference, data, science, engineering, app, c...</td>\n",
       "      <td>16</td>\n",
       "      <td>tell, forget, really, friend, true, know, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>14</td>\n",
       "      <td>difference, data, science, engineering, app, c...</td>\n",
       "      <td>17</td>\n",
       "      <td>increase, painless, instagram, account, best, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>7</td>\n",
       "      <td>pakistan, black, did, war, india, difference, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>government, ban, banning, black, indian, rupee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>1</td>\n",
       "      <td>air, house, beautiful, body, cause, hair, hard...</td>\n",
       "      <td>14</td>\n",
       "      <td>pounds, reduce, quickly, loss, fast, fat, ways...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  lda_topic  \\\n",
       "0  What is the step by step guide to invest in sh...          3   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...         14   \n",
       "2  How can I increase the speed of my internet co...         14   \n",
       "3  Why am I mentally very lonely? How can I solve...          7   \n",
       "4  Which one dissolve in water quikly sugar, salt...          1   \n",
       "\n",
       "                                        lda_keywords  nmf_topic  \\\n",
       "0  stock, english, make, market, friends, college...          5   \n",
       "1  difference, data, science, engineering, app, c...         16   \n",
       "2  difference, data, science, engineering, app, c...         17   \n",
       "3  pakistan, black, did, war, india, difference, ...         11   \n",
       "4  air, house, beautiful, body, cause, hair, hard...         14   \n",
       "\n",
       "                                        nmf_keywords  \n",
       "0  china, business, country, olympics, available,...  \n",
       "1  tell, forget, really, friend, true, know, pers...  \n",
       "2  increase, painless, instagram, account, best, ...  \n",
       "3  government, ban, banning, black, indian, rupee...  \n",
       "4  pounds, reduce, quickly, loss, fast, fat, ways...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##assigning topics to each question alongside keywords\n",
    "\n",
    "nmf_topic_distribution = NMF.transform(tfidf_term_matrix)\n",
    "\n",
    "df['nmf_topic'] = nmf_topic_distribution.argmax(axis=1) ##grab the topic with highest coefficient value\n",
    "df['nmf_keywords'] = df['nmf_topic'].apply(lambda topic: ', '.join(nmf_dct[topic]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d65f6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##random check function for NMF\n",
    "def nmf_check(index=np.random.randint(0,df.shape[0])):\n",
    "    print(f\"Topic {df['nmf_topic'][index]}: {df['nmf_keywords'][index]}.\")\n",
    "    print(\"\\n\")\n",
    "    print(df['Question'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddeef6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 12: girl, 2017, year, don, employees, going, day, things, new, know.\n",
      "\n",
      "\n",
      "I sent a follow request to someone on Instagram. Why hasn't he approved the request for so long? I know that person is posting pictures on instagram based on his increasing number of posts. Is he not getting my request or is he ignoring my request?\n"
     ]
    }
   ],
   "source": [
    "nmf_check(123879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a41de59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 10: business, read, start, job, work, engineering, ways, bad, books, good.\n",
      "\n",
      "\n",
      "If I want to apply cdse. what course should I join at college?\n"
     ]
    }
   ],
   "source": [
    "nmf_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b4d8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating a random check function incorporating both LDA and NMF\n",
    "def random_check(index=np.random.randint(0,df.shape[0])):\n",
    "    print(f\"LDA keywords: {df['lda_keywords'][index]}\")\n",
    "    print(f\"NMF keywords: {df['nmf_keywords'][index]}\")\n",
    "    print('\\n')\n",
    "    print(df['Question'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea0ca31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA keywords: believe, answer, ask, instagram, question, facebook, questions, google, quora, people\n",
      "NMF keywords: improvement, delete, asked, google, answers, answer, ask, question, questions, quora\n",
      "\n",
      "\n",
      "How do I post something on Quora?\n"
     ]
    }
   ],
   "source": [
    "random_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "047b4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA keywords: believe, answer, ask, instagram, question, facebook, questions, google, quora, people\n",
      "NMF keywords: improvement, delete, asked, google, answers, answer, ask, question, questions, quora\n",
      "\n",
      "\n",
      "How do I post something on Quora?\n"
     ]
    }
   ],
   "source": [
    "random_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea942d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
